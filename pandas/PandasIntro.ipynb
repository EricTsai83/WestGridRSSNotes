{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "\n",
    "Pandas (Python Data Analysis Library) is a swiss-army knife module that you'll find at the top of a huge proportion of notebooks. It is so popular that there's even an idiom for how to import it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a quick idea of the scope of pandas take a look at the autocomplete for pd.<TAB>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental objects in pandas are the Series and the DataFrame. Together they encapsulate how you will normally ingest, clean, manipulate and even visualize external data sources. Most of your work will use methods on these \n",
    "objects so we'll take a quick tour of the concepts they implement, and the idea of an Index.\n",
    "\n",
    "## Series\n",
    "The pandas Series object is basically a one dimensional indexed array. Unpacking that, it is something that will look like\n",
    "\n",
    "| Index | Value |\n",
    "|-------|-------|\n",
    "|   0   |  0.12 |\n",
    "|   1   |  0.24 |\n",
    "|   2   |  0.36 |\n",
    "|   3   |  0.48 |\n",
    "\n",
    "One thing to notice is that the values are all of the same time (floating point here). The index is fairly flexible, it might be numbers (as here), strings, timestamps or something else. The main restriction is that they should be hashable. Let's create some series to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sf a series of floats from 1.0 to 5.0\n",
    "\n",
    "# si a series of integers from 0 to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__repr__` includes the values we set along with the type of object we have stored (the values). As I mentioned, this is one of the attributes of a Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check `.dtype` of sf and si"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, this looks a lot like a numpy array (or even just a list), but we can switch the indexing to suit our needs, by explicitly passing the `index=` arguement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm a seris of 5 floats with the index being their values as words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the series is starting to look more like a dictionary, in fact, that's a pretty good way to construct series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sn a series created from a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you look closely though, a Series has a few tricks that a dictionary doesn't..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try slicing sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. label based indexes are _inclusive_ of the `stop` value. This is different from most other indexes you'll see in python. This can be a little confusion, but it basically boils down to the idea that there isn't always a natural \"next\" object in a hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the `.keys` of sn\n",
    "\n",
    "# Check if 'four' is in the series index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element by element statements evaluate to Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where sn > 2\n",
    "\n",
    "\n",
    "# Extract the values where sn > 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some indexing methods available to you `.loc`, `.iloc`, `.ix`. These might seem redundant, but actually they're useful in some contexts where ordinary indexing will bite you..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = pd.Series(data=['apple', 'orange', 'pineapple'], index=[1, 3, 7])\n",
    "sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When actually, you probably meant..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.loc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iloc` does the opposite and tells pandas you want to use the implicit style python notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.iloc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ix` does something similar, but I've never been able to commit it to memory...ðŸ˜€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "Most of the time you will be using DataFrames rather than series, but dataframes can be thought of as collections of Series. They can actually be multidimensional, but for now think of them as a collection of `Series` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame({'floats': sm, 'ints': sn})\n",
    "d1\n",
    "\n",
    "# Check the index and colums of d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When indexing a dataframe, the default is to give you the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the 'ints' column of d1\n",
    "\n",
    "# Check the type of the object you get back from this indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are looking for the row, then try `.loc` with the row index value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the `one` row of d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify columns using the attribute notation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the `.ints` column of d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Columns\n",
    "We can manipulate existing columns or create new ones, but simple transformations will return the transformed object rather than modifying in place. Many options have an `inplace=True` argument, but you can also just reassign the returned DataFrame to the same name.\n",
    "\n",
    "When you slice or subset, pandas will usually try to get away with giving you a view on the existing dataframe rather than giving you a new object. This is cheaper in terms of memory allocated and also performance penalty of the copies, sometimes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double the d1['ints'] column\n",
    "\n",
    "# Check that d1 is unchanged\n",
    "\n",
    "# divide the ints column of d1 by twice the floats column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Attributes\n",
    "\n",
    "We've already seen some of the attributes of the DataFrame (column etc.) but there are quite a few available, take a look at `dtypes`, `ndim`, `shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Methods\n",
    "\n",
    "There are *lots* of methods for operating on DataFrames, have a look at the tab completion and explore the documentation for them. In particular, take a look at the help for `describe`, `head` and `tail`. These are great for orienting yourself with a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the d1 DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with External Data\n",
    "\n",
    "Data comes in many, many forms from simple csv/json files, real-time APIs, structured binary files and many others. Try pd.read_<TAB> to see some of the available options. `read_csv` and `read_json` are the workhorses. We'll start with pd.read_csv which is more fexible but many of the arguments to read_csv will have equivalents for the other functions. `read_json` can be useful for quicky interacting with public APIs which will commonly publish JSON endpoints.\n",
    "    \n",
    "    \n",
    "We need a CSV to work with. The city of Vancouver has an [open data catalog](https://vancouver.ca/your-government/open-data-catalogue.aspx), which has CSV for some of the datasets. You can download the file directly if you want, e.g.\n",
    "    \n",
    "!curl -O ftp://webftp.vancouver.ca/OpenData/csv/CommunityGardensandFoodTrees.csv\n",
    "\n",
    "but `read_csv` will also happily read from remote for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF = pd.read_csv(\n",
    "    \"ftp://webftp.vancouver.ca/OpenData/csv/CommunityGardensandFoodTrees.csv\",\n",
    "    encoding='latin1'\n",
    ")\n",
    "gardenDF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 168 rows, with 19 columns, here are the fist few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know when the gardens were created (`YEAR_CREATED`), and where the are (`LATITUDE`, `LONGITUDE`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to clean the data. This is a hugely important step and in research will eat a lot of your time, but it can be worth it. First let's look at the index, the default is to index by integer, but we could have picked any column instead. It looks like the first column is unique (`MAPID`) and so let's use that (chosing the index right can make your life much easier when adding data or combining multiple DataFrames). The `inplace=True` argument means modify the existing dataframe rather than returning a modified copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `.set_index` to mapID in place\n",
    "\n",
    "# Use the `.unique` method on the `YEAR_CREATED` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pre-2010`, `pre-1970` and `pre 200` are kind of usless (and inconsistent!) so let's toss them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for badLabel in ['Pre-2010', 'pre-1970', 'pre 2000']:\n",
    "    gardenDF = gardenDF[gardenDF['YEAR_CREATED'] != badLabel]\n",
    "\n",
    "# Check the unique values in `YEAR_CREATED` again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's still a `nan`, and the years are strings (numbers would be better or even dates). Pandas is pretty smart about dealing with missing data, but when it gets in the way there are functions like dropna() which will tell pandas to remove them. If we try to convert the `YEAR_CREATED` column to an integer blindly, it will barf on the `nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `.dropna` and `astype` methods to make everything in YEAR_CREATED an integer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice that we still have a `NaN` (formally, `NaN` is a float so our column was \"promoted\"). One of the Big advantages of Pandas is that it will do something sensible with missing data rather than just barfing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF['YEAR_CREATED'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a bit more visual, again notice that the `NaN` is just safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Create a histogram of the YEAR_CREATED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of pandas ingenstion methods will let you do the transformations when you first read in the data by passing arguments to `read_csv` (or `read_json`, ...). Some of the more useful options are\n",
    "\n",
    "  * **delimiter=**: Sometimes a csv is a tsv, tabs are evil\n",
    "  * **names=**: Pass a list of names to use for the columns\n",
    "  * **usecols=**: Only slurp up a subset of columns\n",
    "  * **skiprows=**: Ignore a number of rows at the top of the file\n",
    "  * **na_values=**: Flag values which the CSV author used to indicate missing data, e.g. -1\n",
    "  * **encoding=**: ...\n",
    "  * **converters=**: Do some transformation on the columns before importing them\n",
    "  * **parse_dates**=: Turn strings into dates\n",
    "  \n",
    "For the last two options you normally have to add a bit of logic to help pandas. For dates this might be because you  columns for years, months and days and you need to combine them, or you have to look for a timezone or something. This seems like a hassle, but it is usually worth it. Once you have a column (or an index) as a datetime object you can index it very flexibly (e.g. you can ask for all the rows which fall on a weekend in the PTD timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gardenDF = pd.read_csv(\n",
    "    \"ftp://webftp.vancouver.ca/OpenData/csv/CommunityGardensandFoodTrees.csv\",\n",
    "    encoding='latin1',\n",
    "    na_values={'YEAR_CREATED': ['Pre-2010', 'pre-1970', 'pre 2000', 'nan']},\n",
    "    index_col='MAPID',\n",
    "    dtype={'YEAR_CREATED': float}\n",
    ")\n",
    "gardenDF['YEAR'] = pd.to_datetime(gardenDF['YEAR_CREATED'], format='%Y.0')\n",
    "gardenDF[['YEAR_CREATED', 'YEAR']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the `NAME` of the gardens created before 1950"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeSeries\n",
    "\n",
    "We've already talked about time and date handling a bit, but I use this a lot, so we can talk about it more! Pandas was created to handle Financial data and do financial modeling. This lineage has given pandas really excellent time handling. The main objects are\n",
    "\n",
    "  * **Time Stamps**: Specific points in time usually recorded to the second or nanosecond\n",
    "  * **Time Intervals/Time Deltas**: These types lets you do arithmetic on time objects\n",
    "\n",
    "We need some dates to play with. There's a convenience function called `to_datetime` which can convert many \"human readable\" dates to a pd.Timestamp object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moonwalk = pd.to_datetime('July 20, 1969, 20:17 UTC')\n",
    "moonwalk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timestamps have attributes which let you extract days, year, etc. Normally these will be reported as numbers, but the strftime method supports the usual format specifiers (The correspond with the libc specifiers, here's a reference http://strftime.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The moon walk took place on a {moonwalk.strftime('%A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pd.datetime.now() to find out how long ago the Apollo 11 moon walk was\n",
    "# see also `from dateutil import relativedelta`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another sample dataset, it contains three colums\n",
    "\n",
    "  * year\n",
    "  * month\n",
    "  * passengers\n",
    "  \n",
    "We can combine the year and month to create a date, then we can use the result as the index for a single column dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/flights.csv',\n",
    "    parse_dates=[['year','month']],\n",
    "    index_col='year_month'\n",
    ")\n",
    "flightsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dtypes of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can index based on date. Let's look at the number of flights in the 1951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the flights between 1951 and 1952 and plot as a bar graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timestamp objects can also deal with arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the last and first elements of the index to find out the time range of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One extremely useful feature with time series is the ability to resample existing time series. For example, we could resample the flight data into year long bins and look at how the mean passenger count increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample flightsDF as yearly data, find the mean number of passengers and do another bar plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping, Joining, Concatenating\n",
    "\n",
    "You can get pretty far by jamming everything into a single dataframe, but sometimes you might want to do aggregate operations within a dataframe (e.g. group together all of the rows by year and show the mean value of some other column). Alternatively you might want to add new rows to or columns to an existing DataFrame or join dataframes based on shared key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the following csv into a dataframe called carsDF\n",
    "#  https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mpg.csv\n",
    "\n",
    "\n",
    "# Calculate the mean value of the mpg column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "Let's group things by number_of_cylinders and see how that affects mpg..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby then find the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling groupby on it's own will give you a `DataFrameGroupBy` object, you have to tell it what you want to do with the groups to actually see some results, this can be convenient if you want to look at different aggregate functions on the same groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDFbyCylinders = carsDF.groupby('cylinders')\n",
    "carsDFbyCylinders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical aggregate operations are things like\n",
    "\n",
    "  * mean()\n",
    "  * sum()\n",
    "  * median()\n",
    "  * min()/max()\n",
    "\n",
    "Group the carsDF by model_year and look at the median mpg (don't include the other columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can actually do much more with groupby, you can iterate over the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the carsDFbyCylinders and spit out the number of cars in each group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are there possibly 4 cars with 3 cylinders?!\n",
    "\n",
    "We can apply multiple operations at the same time. The `.aggregate()` method can take a list of the operations you want to perform (e.g. [\"max\", \"min\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter based on group, this is a silly example, but group the cars by cylinder, then show me all of the groups with a mean mpg > 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDFbyCylinders.filter(lambda x: x['mpg'].mean()>15).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transform method lets you perform a group operation then use the results to update the rows. For example, we could calculate mean values for our groups, then look at how individual cars perform relative to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsDFbyCylinders.transform(lambda x: x - x.mean()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also an apply method which is even more general and will let you apply an arbitrary function to the group results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate & Join\n",
    "\n",
    "There are a handful of functions which handle concatenation. The main workhorse is `pd.concat`, but there are some convenience functions which will let you avoid passing lots of arguments to concat. You can combine `Series` and `DataFrames` but we'll jump straight to `DataFrames`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.DataFrame(\n",
    "    {\n",
    "        'upper': ['A', 'B', 'C'], \n",
    "        'lower': ['a', 'b', 'c']\n",
    "    }, \n",
    "    columns=['upper', 'lower'], \n",
    "    index=[1,2,3]\n",
    ")\n",
    "\n",
    "s2 = pd.DataFrame(\n",
    "    {\n",
    "        'upper': ['D', 'E', 'F'],\n",
    "        'lower': ['d', 'e', 'f']\n",
    "    }, \n",
    "    columns=['upper', 'lower'],\n",
    "    index=[4,5,6]\n",
    ")\n",
    "\n",
    "pd.concat([s1,s2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we enclosed the thigs we want to join as some sort of iterable (a `list` here). What happens if one of the columns is missing from the data frame?\n",
    "\n",
    "What if we wanted to add columns rather than rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.DataFrame({'upper': ['A', 'B', 'C'], 'lower': ['a', 'b', 'c']}, columns=['upper', 'lower'], index=[1,2,3])\n",
    "t2 = pd.DataFrame({'greek': ['Î±', 'Î²', 'Î³']}, index=[1,2,3])\n",
    "\n",
    "pd.concat([t1, t2], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.concat` will accept duplicate indices, but normally that indicates a problem with the data normalization. `concat` has a `verify_index` argument which can check for these problems and you can specify what you want to do with duplicates manually.\n",
    "\n",
    "concat will often result in `NaN`s because some columns might not exist in both/all frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = pd.DataFrame({'fruit': ['apple', 'orange'], 'veg': ['brocolli', 'carrot'], 'tree': ['cedar', 'alder']})\n",
    "d5 = pd.DataFrame({'veg': ['onion', 'potato'], 'fruit': ['banana','grape']})\n",
    "pd.concat([d4,d5], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the index wasn't important here, I threw it away and just accepted a new one.\n",
    "\n",
    "In the general case, joining DataFrames can get complex. The concat method can take a `join` keyword to specify a database like join stragegy (inner or outer), but `pd.merge` is a bit more flexible. It implements the usual relations\n",
    "\n",
    "  * one-to-one (similar to a concat)\n",
    "  * many-to-one\n",
    "  * many-to-many\n",
    "  \n",
    "For the many-to-one case here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf1=pd.DataFrame({\n",
    "    'class': ['insect', 'spider'], \n",
    "    'legs': [6, 8]}\n",
    ")\n",
    "\n",
    "adf2=pd.DataFrame({\n",
    "    'name': ['molly', 'anna', 'stephen', 'mica'], \n",
    "    'class': ['insect','insect','spider','insect']}\n",
    ")                     \n",
    "\n",
    "print(adf1); print(adf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(adf1, adf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _many-to-one_ is many different rows in adf2 being mapped to a single row in adf1 (insects). `pd.merge` also accepts a selection of keyword arguments so you can manually specify which columns to join, patch up name differences etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
